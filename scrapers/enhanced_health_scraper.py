#!/usr/bin/env python3
"""
Enhanced Health News Scraper with Multiple APIs and URL Validation

This scraper uses multiple reliable data sources and validates all URLs
before saving to ensure high-quality, accessible content.
"""

import sys
from pathlib import Path
import requests
import sqlite3
import json
import re
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import time
import logging

# Add project root to path
BASE_DIR = Path(__file__).resolve().parent.parent
sys.path.append(str(BASE_DIR))

# Import our URL validator
try:
    # Add the parent directory to path for app imports
    sys.path.insert(0, str(BASE_DIR))
    from app.url_validator import URLValidator
except ImportError:
    # Fallback if URL validator is not available
    class URLValidator:
        def validate_article_url(self, article):
            return True, {"status": "valid"}

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Database path
DB_PATH = BASE_DIR / "db" / "articles.db"

class EnhancedHealthScraper:
    """Enhanced scraper with multiple APIs and URL validation"""
    
    def __init__(self):
        self.url_validator = URLValidator()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # API Keys (you can add these to environment variables)
        self.newsapi_key = "your_newsapi_key_here"  # Get from https://newsapi.org/
        self.guardian_api_key = "your_guardian_key_here"  # Get from The Guardian
        
        # Health-focused RSS feeds (most reliable)
        self.reliable_rss_feeds = [
            {
                "url": "https://www.nih.gov/news-events/news-releases/rss.xml",
                "name": "NIH News",
                "priority": "high"
            },
            {
                "url": "https://www.cdc.gov/rss/health.xml",
                "name": "CDC Health",
                "priority": "high"
            },
            {
                "url": "https://www.who.int/rss-feeds/news-english.xml",
                "name": "WHO News",
                "priority": "high"
            },
            {
                "url": "https://feeds.reuters.com/reuters/healthNews",
                "name": "Reuters Health",
                "priority": "medium"
            },
            {
                "url": "https://rss.cnn.com/rss/edition_health.rss",
                "name": "CNN Health",
                "priority": "medium"
            },
            {
                "url": "https://feeds.washingtonpost.com/rss/national/health-science",
                "name": "Washington Post Health",
                "priority": "medium"
            }
        ]
        
    def create_database(self):
        """Create enhanced database schema"""
        DB_PATH.parent.mkdir(exist_ok=True)
        
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                date TEXT,
                title TEXT,
                authors TEXT,
                summary TEXT,
                url TEXT UNIQUE,
                categories TEXT,
                tags TEXT,
                source TEXT,
                priority INTEGER DEFAULT 1,
                url_health TEXT,
                url_accessible INTEGER DEFAULT 1,
                last_checked TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # Create index for faster queries
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_url_accessible ON articles(url_accessible)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_date ON articles(date)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_priority ON articles(priority)")
        
        conn.commit()
        conn.close()
        logger.info(f"‚úÖ Enhanced database ready at: {DB_PATH}")
    
    def fetch_newsapi_health(self, max_articles: int = 50) -> List[Dict]:
        """Fetch from NewsAPI (requires API key)"""
        if self.newsapi_key == "your_newsapi_key_here":
            logger.info("‚ö†Ô∏è NewsAPI key not configured, skipping NewsAPI")
            return []
        
        articles = []
        
        try:
            # Health-related queries
            queries = [
                "health", "medical breakthrough", "disease prevention", 
                "nutrition research", "mental health", "public health"
            ]
            
            for query in queries[:2]:  # Limit to avoid rate limits
                url = f"https://newsapi.org/v2/everything"
                params = {
                    'q': query,
                    'language': 'en',
                    'sortBy': 'publishedAt',
                    'pageSize': min(25, max_articles // len(queries)),
                    'from': (datetime.now() - timedelta(days=7)).isoformat(),
                    'apiKey': self.newsapi_key
                }
                
                response = self.session.get(url, params=params)
                response.raise_for_status()
                
                data = response.json()
                
                for article in data.get('articles', []):
                    if article.get('url') and article.get('title'):
                        article_data = {
                            'title': article['title'],
                            'summary': article.get('description', ''),
                            'url': article['url'],
                            'date': article.get('publishedAt', datetime.now().isoformat()),
                            'author': article.get('author', ''),
                            'source': f"NewsAPI ({article.get('source', {}).get('name', 'Unknown')})",
                            'priority': 2
                        }
                        articles.append(article_data)
                
                time.sleep(1)  # Rate limiting
                
        except Exception as e:
            logger.error(f"Error fetching from NewsAPI: {e}")
        
        return articles
    
    def parse_rss_feed(self, feed_config: Dict) -> List[Dict]:
        """Parse RSS feed with error handling"""
        articles = []
        
        try:
            response = self.session.get(feed_config['url'], timeout=15)
            response.raise_for_status()
            
            from xml.etree import ElementTree as ET
            root = ET.fromstring(response.content)
            
            items = root.findall('.//item') or root.findall('.//entry')
            
            for item in items:
                try:
                    title_elem = item.find('title')
                    desc_elem = item.find('description') or item.find('summary')
                    link_elem = item.find('link')
                    date_elem = item.find('pubDate') or item.find('published')
                    
                    if title_elem is not None and link_elem is not None:
                        # Clean description
                        description = ""
                        if desc_elem is not None:
                            description = desc_elem.text or ""
                            description = re.sub(r'<[^>]+>', '', description)
                            description = re.sub(r'\s+', ' ', description).strip()[:500]
                        
                        # Get link
                        link = link_elem.text or link_elem.get('href', '')
                        
                        # Parse date
                        article_date = datetime.now()
                        if date_elem is not None and date_elem.text:
                            try:
                                date_str = date_elem.text.strip()
                                date_str = re.sub(r'\s+[A-Z]{3,4}$', '', date_str)
                                article_date = datetime.strptime(date_str.split(' +')[0], '%a, %d %b %Y %H:%M:%S')
                            except:
                                pass
                        
                        # Only recent articles (last 7 days)
                        if article_date > datetime.now() - timedelta(days=7):
                            priority = 3 if feed_config['priority'] == 'high' else 2 if feed_config['priority'] == 'medium' else 1
                            
                            article_data = {
                                'title': title_elem.text.strip(),
                                'summary': description,
                                'url': link.strip(),
                                'date': article_date.isoformat(),
                                'author': '',
                                'source': feed_config['name'],
                                'priority': priority
                            }
                            articles.append(article_data)
                            
                except Exception as e:
                    logger.warning(f"Error parsing RSS item from {feed_config['name']}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error fetching RSS feed {feed_config['name']}: {e}")
        
        return articles
    
    def categorize_article(self, article: Dict) -> Dict:
        """Enhanced categorization"""
        text = f"{article['title']} {article['summary']}".lower()
        
        # Breaking news indicators
        breaking_words = ['breaking', 'urgent', 'alert', 'emergency', 'critical', 'just in']
        if any(word in text for word in breaking_words):
            return {
                'categories': ['news'],
                'tags': ['breaking_news']
            }
        
        # Disease categories
        disease_words = [
            'cancer', 'diabetes', 'heart disease', 'covid', 'flu', 'infection', 
            'disease', 'illness', 'condition', 'syndrome', 'disorder'
        ]
        if any(word in text for word in disease_words):
            return {
                'categories': ['diseases'],
                'tags': ['medical_conditions']
            }
        
        # Treatment/solutions
        treatment_words = [
            'treatment', 'therapy', 'medication', 'drug', 'surgery', 'cure', 
            'vaccine', 'medicine', 'clinical trial', 'fda approval'
        ]
        if any(word in text for word in treatment_words):
            return {
                'categories': ['solutions'],
                'tags': ['medical_treatments']
            }
        
        # Nutrition/food
        food_words = [
            'diet', 'nutrition', 'food', 'vitamin', 'supplement', 'eating', 
            'meal', 'healthy eating', 'obesity', 'weight loss'
        ]
        if any(word in text for word in food_words):
            return {
                'categories': ['food'],
                'tags': ['nutrition_basics']
            }
        
        # Research/trending
        research_words = [
            'study', 'research', 'clinical trial', 'findings', 'breakthrough', 
            'discovery', 'published', 'journal', 'scientists'
        ]
        if any(word in text for word in research_words):
            return {
                'categories': ['trending_topics'],
                'tags': ['breakthrough_research']
            }
        
        # Default to news
        return {
            'categories': ['news'],
            'tags': ['recent_developments']
        }
    
    def save_validated_articles(self, articles: List[Dict], source_name: str) -> int:
        """Save articles with URL validation"""
        if not articles:
            logger.info(f"‚ö†Ô∏è No articles to save from {source_name}")
            return 0
        
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        saved_count = 0
        invalid_count = 0
        duplicate_count = 0
        
        for article in articles:
            try:
                # Validate URL before saving
                is_valid, health_info = self.url_validator.validate_article_url(article)
                
                if not is_valid:
                    invalid_count += 1
                    logger.warning(f"Skipping invalid URL: {article['title'][:50]}... - {health_info.get('error', 'Unknown error')}")
                    continue
                
                # Categorize article
                categorization = self.categorize_article(article)
                article.update(categorization)
                
                # Convert lists to JSON
                categories_json = json.dumps(article.get('categories', ['news']))
                tags_json = json.dumps(article.get('tags', ['general']))
                
                cursor.execute("""
                    INSERT OR IGNORE INTO articles 
                    (date, title, authors, summary, url, categories, tags, source, priority, url_health, url_accessible, last_checked)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    article['date'],
                    article['title'],
                    article.get('author', ''),
                    article['summary'],
                    article['url'],
                    categories_json,
                    tags_json,
                    article.get('source', source_name),
                    article.get('priority', 1),
                    json.dumps(health_info),
                    1 if is_valid else 0,
                    datetime.now().isoformat()
                ))
                
                if cursor.rowcount > 0:
                    saved_count += 1
                    logger.info(f"‚úÖ Saved: {article['title'][:60]}...")
                else:
                    duplicate_count += 1
                    
            except Exception as e:
                logger.error(f"Error saving article: {e}")
        
        conn.commit()
        conn.close()
        
        logger.info(f"üìä {source_name}: {saved_count} saved, {duplicate_count} duplicates, {invalid_count} invalid URLs")
        return saved_count
    
    def fetch_all_sources(self, max_articles_per_source: int = 30) -> int:
        """Fetch from all available sources"""
        total_saved = 0
        start_time = datetime.now()
        
        logger.info("üîç Fetching from multiple enhanced health sources...")
        
        # Fetch from reliable RSS feeds
        logger.info("\nüì∞ Fetching from reliable RSS feeds...")
        for feed_config in self.reliable_rss_feeds:
            articles = self.parse_rss_feed(feed_config)
            saved = self.save_validated_articles(articles, feed_config['name'])
            total_saved += saved
            time.sleep(2)  # Be respectful
        
        # Fetch from NewsAPI (if configured)
        logger.info("\nüîé Fetching from NewsAPI...")
        newsapi_articles = self.fetch_newsapi_health(max_articles_per_source)
        total_saved += self.save_validated_articles(newsapi_articles, "NewsAPI")
        
        duration = datetime.now() - start_time
        
        logger.info(f"\nüìä Enhanced Scraping Summary:")
        logger.info(f"   ‚Ä¢ Total articles saved: {total_saved}")
        logger.info(f"   ‚Ä¢ Duration: {duration.total_seconds():.1f} seconds")
        logger.info(f"   ‚Ä¢ All URLs validated and accessible")
        logger.info(f"   ‚Ä¢ Database: {DB_PATH}")
        
        return total_saved

def main():
    """Main scraping function with enhanced features"""
    print("üß† Enhanced Health News Scraper with URL Validation")
    print("=" * 60)
    
    scraper = EnhancedHealthScraper()
    
    # Create database
    scraper.create_database()
    
    # Fetch from all sources
    total_saved = scraper.fetch_all_sources()
    
    if total_saved > 0:
        print(f"\nüéâ Successfully scraped {total_saved} validated health articles!")
        print("‚úÖ All URLs are verified and accessible!")
        print("\nüí° Next steps:")
        print("   1. Start the API: cd api && uvicorn main:app --reload")
        print("   2. Visit: http://localhost:8000/articles/latest")
        print("   3. Start frontend: cd frontend && npm start")
        print("\nüîß To use additional APIs:")
        print("   ‚Ä¢ Get NewsAPI key: https://newsapi.org/")
        print("   ‚Ä¢ Update the API key in this file")
    else:
        print("\n‚ö†Ô∏è No articles were saved. This might be due to:")
        print("   ‚Ä¢ Network connectivity issues")
        print("   ‚Ä¢ All URLs being invalid or inaccessible")
        print("   ‚Ä¢ Rate limiting from news sources")
    
    return total_saved

if __name__ == "__main__":
    main()